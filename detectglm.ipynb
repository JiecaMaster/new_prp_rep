{
 "cells": [
  {
   "cell_type": "code",
   "id": "f61977d3326d810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-08T12:02:17.370406Z",
     "start_time": "2025-02-08T12:02:15.893391Z"
    }
   },
   "source": [
    "import subprocess\n",
    "import os\n",
    "import time\n",
    "from typing import List, TypedDict, Dict\n",
    "from typing_extensions import Annotated\n",
    "\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = \"d9ae4d81e22cb9483f5b4d875ba2d1c1.0QNkBJCwDX7rLdI9\"\n",
    "# 用于并行执行 agent 任务\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# 导入 LangGraph 相关组件\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "# 初始化ChatZhipuAI模型\n",
    "llm = ChatZhipuAI(\n",
    "    model=\"glm-4-plus\",\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")"
   ],
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "# =============================================================================\n",
    "# 定义整体状态数据结构\n",
    "# =============================================================================\n",
    "# 初始状态中包含：\n",
    "# - pcap_file：待分析的 pcap 文件路径；\n",
    "# - log_files：节点1生成的 .log 文件列表；\n",
    "# - txt_files：节点2生成的 .txt 文件列表；\n",
    "# - agent_tasks：经过筛选和拆分后产生的任务列表，每个任务对应一个 agent 的处理工作，\n",
    "#       任务示例格式：{\"agent_id\": int, \"source_txt\": str}\n",
    "# - messages 字段由 add_messages 管理（但每个 agent在处理时会使用独立的对话内存）\n",
    "class State(TypedDict):\n",
    "    pcap_file: str\n",
    "    log_files: List[str]\n",
    "    txt_files: List[str]\n",
    "    agent_tasks: List[Dict]  # 每个任务为 {\"agent_id\": int, \"source_txt\": str}\n",
    "    messages: Annotated[List[dict], add_messages]  # 全局对话历史（此处仅作占位，不用于 agent 间共享）\n",
    "\n",
    "# =============================================================================\n",
    "# 创建状态图构建器\n",
    "# =============================================================================\n",
    "graph_builder = StateGraph(State)"
   ],
   "id": "9d583fcaa50a13f7"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "\n",
    "###############################################################################\n",
    "# 节点1：调用第一个 shell 脚本，利用 zeek 分析指定 pcap 文件，\n",
    "#         生成多个 .log 文件（模拟数据）\n",
    "###############################################################################\n",
    "def analyze_pcap(state: State) -> State:\n",
    "    print(\"【节点1】调用 sh 脚本运行 zeek 分析 pcap 文件...\")\n",
    "    # TODO: 根据实际情况调用 shell 脚本，例如：\n",
    "    # subprocess.run([\"sh\", \"analyze_pcap.sh\", state[\"pcap_file\"]], check=True)\n",
    "    #\n",
    "    # 模拟生成 .log 文件列表\n",
    "    state[\"log_files\"] = [\"capture1.log\", \"capture2.log\", \"capture3.log\"]\n",
    "    print(f\"生成的 log 文件：{state['log_files']}\")\n",
    "    return state\n",
    "\n",
    "graph_builder.add_node(\"analyze_pcap\", analyze_pcap)\n",
    "graph_builder.set_entry_point(\"analyze_pcap\")"
   ],
   "id": "51ddf4b7a2c30b72"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###############################################################################\n",
    "# 节点2：调用第二个 shell 脚本，将 .log 文件转换为 .txt 文件，\n",
    "#         并删除每个文件的前 n 行与最后 m 行（模拟数据）\n",
    "###############################################################################\n",
    "def convert_logs(state: State) -> State:\n",
    "    print(\"【节点2】调用 sh 脚本转换 .log 文件为 .txt 文件，并删除前 n 行与最后 m 行...\")\n",
    "    # TODO: 遍历 state[\"log_files\"]，调用转换脚本，例如：\n",
    "    # subprocess.run([\"sh\", \"convert_logs.sh\", log_file, str(n), str(m)], check=True)\n",
    "    #\n",
    "    # 模拟转换：假设部分转换后的 txt 文件有效（文件名包含 \"useful\"），部分无效\n",
    "    simulated_txt = []\n",
    "    for log in state[\"log_files\"]:\n",
    "        # 模拟转换结果：将 capture1.log、capture3.log 转换为有用的文件\n",
    "        if \"capture1\" in log or \"capture3\" in log:\n",
    "            simulated_txt.append(log.replace(\".log\", \"_useful.txt\"))\n",
    "        else:\n",
    "            simulated_txt.append(log.replace(\".log\", \"_useless.txt\"))\n",
    "    state[\"txt_files\"] = simulated_txt\n",
    "    print(f\"转换得到的 txt 文件：{state['txt_files']}\")\n",
    "    return state\n",
    "\n",
    "graph_builder.add_node(\"convert_logs\", convert_logs)\n",
    "graph_builder.add_edge(\"analyze_pcap\", \"convert_logs\")"
   ],
   "id": "836c354aff640240"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###############################################################################\n",
    "# 节点3：筛选有效的 txt 文件，并基于每个有效文件生成任务列表\n",
    "#         （注意：一个 txt 文件可能生成多个任务，即对应多个 agent）\n",
    "###############################################################################\n",
    "def filter_and_split_tasks(state: State) -> State:\n",
    "    print(\"【节点3】筛选有效 txt 文件，并生成 agent 任务列表...\")\n",
    "    agent_tasks = []\n",
    "    agent_id_counter = 1\n",
    "    for txt in state[\"txt_files\"]:\n",
    "        # 仅选取文件名中包含 \"useful\" 的文件\n",
    "        if \"useful\" in txt:\n",
    "            # TODO: 根据实际需求，可能对同一个 txt 文件生成多个任务\n",
    "            # 例如：如果文件内容包含多个独立数据块，则每个数据块对应一个任务\n",
    "            # 这里模拟：对于每个有效文件，随机生成 1~2 个任务\n",
    "            # 模拟：若文件名中含 \"capture1\", 生成 2 个任务；否则 1 个任务\n",
    "            num_tasks = 2 if \"capture1\" in txt else 1\n",
    "            for _ in range(num_tasks):\n",
    "                task = {\"agent_id\": agent_id_counter, \"source_txt\": txt}\n",
    "                agent_tasks.append(task)\n",
    "                print(f\"生成任务：{task}\")\n",
    "                agent_id_counter += 1\n",
    "    state[\"agent_tasks\"] = agent_tasks\n",
    "    print(f\"共计生成 {len(agent_tasks)} 个 agent 任务\")\n",
    "    return state\n",
    "\n",
    "graph_builder.add_node(\"filter_and_split_tasks\", filter_and_split_tasks)\n",
    "graph_builder.add_edge(\"convert_logs\", \"filter_and_split_tasks\")"
   ],
   "id": "756f45a22941e542"
  },
  {
   "metadata": {
    "collapsed": true
   },
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "###############################################################################\n",
    "# 节点4：并行处理所有 agent 任务\n",
    "#         每个任务独立进行：\n",
    "#           ① 对应的 txt 文件处理生成 y1.txt；\n",
    "#           ② 将 y1.txt 切分为每 30 行一个数据块；\n",
    "#           ③ 针对每个数据块调用 llm.invoke（带独立记忆），问答结果写入 answer_agent_x.txt；\n",
    "#           ④ 动态显示处理进度。\n",
    "###############################################################################\n",
    "def process_agent_task(task: Dict) -> None:\n",
    "    agent_id = task[\"agent_id\"]\n",
    "    source_txt = task[\"source_txt\"]\n",
    "    agent_name = f\"agent_{agent_id}\"\n",
    "    print(f\"[{agent_name}] 开始处理任务，源文件：{source_txt}\")\n",
    "\n",
    "    # ① 模拟对源 txt 进行处理，生成 y1.txt（实际请在此处实现字段提取逻辑）\n",
    "    y1_file = f\"{agent_name}_y1.txt\"\n",
    "    # 模拟读取源文件内容（实际中应读取 source_txt 文件）\n",
    "    simulated_content = \"\\n\".join([f\"Line {i+1} from {source_txt}\" for i in range(150)])\n",
    "    # TODO: 在此处处理 simulated_content 保留特定字段，生成 y1_file 内容\n",
    "    y1_content = simulated_content  # 模拟处理后结果\n",
    "    with open(y1_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(y1_content)\n",
    "    print(f\"[{agent_name}] 处理生成 {y1_file}\")\n",
    "\n",
    "    # ② 按每 30 行切分 y1.txt 内容为多个数据块\n",
    "    lines = y1_content.splitlines()\n",
    "    data_chunks = [\"\\n\".join(lines[i:i+30]) for i in range(0, len(lines), 30)]\n",
    "    num_chunks = len(data_chunks)\n",
    "    print(f\"[{agent_name}] 切分为 {num_chunks} 个数据块\")\n",
    "\n",
    "    # ③ 针对每个数据块进行 LLM 问答，每个 agent 使用独立的对话记忆\n",
    "    conversation_memory = []  # 独立的对话记忆列表\n",
    "    answer_file = f\"answer_{agent_name}.txt\"\n",
    "    with open(answer_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for idx, chunk in enumerate(data_chunks):\n",
    "            if idx == 0:\n",
    "                prompt = f\"语句A: {chunk}\"\n",
    "            else:\n",
    "                prompt = f\"语句B: {chunk}\"\n",
    "            # 添加用户提问到独立记忆中\n",
    "            conversation_memory.append({\"role\": \"user\", \"content\": prompt})\n",
    "            # 调用 LLM 进行问答，使用 invoke 方法传入当前对话记忆\n",
    "            response = llm.invoke(conversation_memory)\n",
    "            # 将 LLM 响应也加入对话记忆\n",
    "            conversation_memory.append({\"role\": \"assistant\", \"content\": response})\n",
    "            # 将回答写入答案文件\n",
    "            f_out.write(response + \"\\n\")\n",
    "            # 动态显示进度\n",
    "            print(f\"[{agent_name}] 已处理数据块 {idx+1}/{num_chunks}\")\n",
    "            f_out.flush()\n",
    "            time.sleep(0.1)  # 模拟处理延时\n",
    "    print(f\"[{agent_name}] 问答结果已保存至 {answer_file}\")\n",
    "\n",
    "def process_agents_parallel(state: State) -> State:\n",
    "    print(\"【节点4】并行处理所有 agent 任务...\")\n",
    "    tasks = state[\"agent_tasks\"]\n",
    "    # 使用线程池并行执行每个 agent 任务\n",
    "    with ThreadPoolExecutor(max_workers=len(tasks)) as executor:\n",
    "        futures = {executor.submit(process_agent_task, task): task for task in tasks}\n",
    "        for future in as_completed(futures):\n",
    "            # 如果任务内部抛出异常，可在此捕获处理\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                task = futures[future]\n",
    "                print(f\"[agent_{task['agent_id']}] 任务处理异常：{e}\")\n",
    "    print(\"所有 agent 任务均已处理完成。\")\n",
    "    return state\n",
    "\n",
    "graph_builder.add_node(\"process_agents_parallel\", process_agents_parallel)\n",
    "graph_builder.add_edge(\"filter_and_split_tasks\", \"process_agents_parallel\")\n",
    "\n",
    "###############################################################################\n",
    "# 设置流程结束点\n",
    "###############################################################################\n",
    "graph_builder.set_finish_point(END)\n",
    "\n",
    "###############################################################################\n",
    "# 编译并运行整个流程图（接入 MemorySaver 检查点，用于 LLM 调用记忆，如有需要）\n",
    "###############################################################################\n",
    "memory = MemorySaver()\n",
    "graph = graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # 初始化状态，仅需提供 pcap 文件路径，后续流程中各字段逐步填充\n",
    "    initial_state: State = {\"pcap_file\": \"input.pcap\"}\n",
    "    graph.run(initial_state)\n"
   ],
   "id": "initial_id"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
