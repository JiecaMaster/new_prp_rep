{
 "cells": [
  {
   "cell_type": "code",
   "id": "f61977d3326d810c",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-02-09T08:26:58.954976Z",
     "start_time": "2025-02-09T08:26:57.947831Z"
    }
   },
   "source": [
    "import os\n",
    "import subprocess\n",
    "import time\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "\n",
    "# ===============================\n",
    "# 配置 API Key（请替换为实际 Key）\n",
    "# ===============================\n",
    "os.environ[\"ZHIPUAI_API_KEY\"] = \"d9ae4d81e22cb9483f5b4d875ba2d1c1.0QNkBJCwDX7rLdI9\"\n",
    "\n",
    "# ===============================\n",
    "# 下面为问答部分采用 langgraph 调用 LLM\n",
    "# ===============================\n",
    "from typing import Annotated\n",
    "from typing_extensions import TypedDict\n",
    "from langgraph.graph import StateGraph, START, END\n",
    "from langgraph.graph.message import add_messages\n",
    "from langchain_community.chat_models import ChatZhipuAI\n",
    "from langchain_core.callbacks.manager import CallbackManager\n",
    "from langchain_core.callbacks.streaming_stdout import StreamingStdOutCallbackHandler\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "\n",
    "# 定义问答状态结构\n",
    "class QAState(TypedDict):\n",
    "    messages: Annotated[list, add_messages]\n",
    "\n",
    "# 构建 langgraph 状态图（用于单次问答调用）\n",
    "qa_graph_builder = StateGraph(QAState)\n",
    "\n",
    "# 初始化 ChatZhipuAI 模型（参数根据实际情况调整）\n",
    "chat = ChatZhipuAI(\n",
    "    model=\"glm-4-plus\",\n",
    "    temperature=0.5,\n",
    "    streaming=True,\n",
    "    callback_manager=CallbackManager([StreamingStdOutCallbackHandler()])\n",
    ")\n",
    "\n",
    "# 定义问答节点函数，调用模型进行问答\n",
    "def qa_node(state: QAState) -> QAState:\n",
    "    response = chat.invoke(state[\"messages\"])\n",
    "    return {\"messages\": [response]}\n",
    "\n",
    "qa_graph_builder.add_node(\"qa\", qa_node)\n",
    "qa_graph_builder.add_edge(START, \"qa\")\n",
    "qa_graph_builder.add_edge(\"qa\", END)\n",
    "memory = MemorySaver()\n",
    "qa_app = qa_graph_builder.compile(checkpointer=memory)\n",
    "\n",
    "def qa_invoke(messages: list) -> object:\n",
    "    \"\"\"\n",
    "    使用 langgraph 调用 LLM 进行问答。\n",
    "    输入：对话历史 messages（列表，每个元素为字典，格式为 {\"role\": ..., \"content\": ...}）\n",
    "    返回：模型输出（通常为最后一次问答响应）\n",
    "    \"\"\"\n",
    "    input_state = {\"messages\": messages}\n",
    "    output_state = qa_app.invoke(input_state)\n",
    "    # 取返回的最后一条消息作为本次响应\n",
    "    return output_state[\"messages\"][-1]\n",
    "\n",
    "# ===============================\n",
    "# 以下为流水线其他部分（均为常规 Python 代码）\n",
    "# ===============================\n",
    "\n",
    "def analyze_pcap(pcap_file: str) -> list:\n",
    "    \"\"\"\n",
    "    调用 shell 脚本运行 zeek 分析指定 pcap 文件，生成多个 .log 文件。\n",
    "    返回生成的 .log 文件列表。\n",
    "    \"\"\"\n",
    "    print(\"【Step 1】分析 pcap 文件...\")\n",
    "    # TODO: 替换为实际调用命令，如：\n",
    "    # subprocess.run([\"sh\", \"analyze_pcap.sh\", pcap_file], check=True)\n",
    "    # 这里模拟返回结果：\n",
    "    log_files = [\"capture1.log\", \"capture2.log\", \"capture3.log\"]\n",
    "    print(f\"生成的 log 文件：{log_files}\")\n",
    "    return log_files\n",
    "\n",
    "def convert_logs(log_files: list, n: int, m: int) -> list:\n",
    "    \"\"\"\n",
    "    调用 shell 脚本，将 .log 文件转换为 .txt 文件，并删除每个文件的前 n 行与最后 m 行。\n",
    "    返回转换得到的 .txt 文件列表。\n",
    "    \"\"\"\n",
    "    print(\"【Step 2】转换 .log 文件为 .txt 文件...\")\n",
    "    txt_files = []\n",
    "    for log in log_files:\n",
    "        # TODO: 替换为实际调用，如：\n",
    "        # subprocess.run([\"sh\", \"convert_logs.sh\", log, str(n), str(m)], check=True)\n",
    "        # 模拟转换结果：例如只有文件名包含 \"capture1\" 和 \"capture3\" 的文件为有效\n",
    "        if \"capture1\" in log or \"capture3\" in log:\n",
    "            txt_files.append(log.replace(\".log\", \"_useful.txt\"))\n",
    "        else:\n",
    "            txt_files.append(log.replace(\".log\", \"_useless.txt\"))\n",
    "    print(f\"转换得到的 txt 文件：{txt_files}\")\n",
    "    return txt_files\n",
    "\n",
    "def filter_effective_txt(txt_files: list) -> list:\n",
    "    \"\"\"\n",
    "    筛选出有效的 .txt 文件（例如文件名中包含 \"useful\"）。\n",
    "    返回有效文件列表。\n",
    "    \"\"\"\n",
    "    effective = [f for f in txt_files if \"useful\" in f]\n",
    "    print(f\"筛选出有效 txt 文件：{effective}\")\n",
    "    return effective\n",
    "\n",
    "def process_txt_file(txt_file: str) -> list:\n",
    "    \"\"\"\n",
    "    处理 txt 文件，提取特定字段后生成新的 y1 文件。\n",
    "    由于一个 txt 文件可能对应多个 y1 文件（即多个任务），\n",
    "    返回 y1 文件路径列表。\n",
    "    \"\"\"\n",
    "    print(f\"【Step 3】处理 {txt_file}，提取字段生成 y1 文件...\")\n",
    "    # TODO: 实现字段提取逻辑，此处模拟：\n",
    "    # 假设如果文件名中含 \"capture1\" 则生成 2 个任务，否则生成 1 个任务\n",
    "    if \"capture1\" in txt_file:\n",
    "        y1_files = [txt_file.replace(\"_useful.txt\", f\"_y1_{i}.txt\") for i in [1, 2]]\n",
    "    else:\n",
    "        y1_files = [txt_file.replace(\"_useful.txt\", \"_y1.txt\")]\n",
    "    print(f\"生成的 y1 文件：{y1_files}\")\n",
    "    return y1_files\n",
    "\n",
    "def split_into_chunks(file_path: str, lines_per_chunk: int = 30) -> list:\n",
    "    \"\"\"\n",
    "    将指定文件内容按每 lines_per_chunk 行切分为数据块。\n",
    "    返回数据块列表（每个数据块为字符串）。\n",
    "    \"\"\"\n",
    "    print(f\"【Step 4】读取并切分文件 {file_path} ...\")\n",
    "    try:\n",
    "        with open(file_path, \"r\", encoding=\"utf-8\") as f:\n",
    "            content = f.read()\n",
    "    except Exception:\n",
    "        # 如果文件不存在（模拟），则生成模拟内容\n",
    "        content = \"\\n\".join([f\"Line {i+1} from {file_path}\" for i in range(150)])\n",
    "    lines = content.splitlines()\n",
    "    chunks = [\"\\n\".join(lines[i:i+lines_per_chunk]) for i in range(0, len(lines), lines_per_chunk)]\n",
    "    print(f\"文件 {file_path} 切分为 {len(chunks)} 个数据块\")\n",
    "    return chunks\n",
    "\n",
    "def process_agent(agent_id: int, y1_file: str) -> None:\n",
    "    \"\"\"\n",
    "    对单个 agent 任务进行问答处理：\n",
    "      ① 从 y1_file 读取内容（或使用模拟数据）；\n",
    "      ② 将内容按每 30 行切分为数据块；\n",
    "      ③ 对每个数据块构造问答提示：首个数据块用 \"语句A: ...\"，后续用 \"语句B: ...\"；\n",
    "      ④ 每次问答调用 qa_invoke，使用独立的对话记忆；\n",
    "      ⑤ 将所有回答写入 answer_agent_{agent_id}.txt，同时在终端动态显示进度。\n",
    "    \"\"\"\n",
    "    agent_name = f\"agent_{agent_id}\"\n",
    "    print(f\"[{agent_name}] 开始处理任务，源文件：{y1_file}\")\n",
    "    # 模拟读取 y1_file 内容（实际中请读取文件）\n",
    "    try:\n",
    "        with open(y1_file, \"r\", encoding=\"utf-8\") as f:\n",
    "            y1_content = f.read()\n",
    "    except Exception:\n",
    "        # 模拟内容\n",
    "        y1_content = \"\\n\".join([f\"Processed line {i+1} from {y1_file}\" for i in range(150)])\n",
    "    chunks = split_into_chunks(y1_file)  # 或直接基于 y1_content 切分\n",
    "    # 若需要根据 y1_content 切分，可用： chunks = [ ... ]（此处简化为调用 split_into_chunks 函数）\n",
    "    num_chunks = len(chunks)\n",
    "    conversation_memory = []  # 每个 agent 独立的对话记忆\n",
    "    answer_file = f\"answer_{agent_name}.txt\"\n",
    "    with open(answer_file, \"w\", encoding=\"utf-8\") as f_out:\n",
    "        for idx, chunk in enumerate(chunks):\n",
    "            if idx == 0:\n",
    "                prompt = f\"语句A: {chunk}\"\n",
    "            else:\n",
    "                prompt = f\"语句B: {chunk}\"\n",
    "            # 将用户提问加入对话记忆\n",
    "            conversation_memory.append({\"role\": \"user\", \"content\": prompt})\n",
    "            # 调用 langgraph 问答接口\n",
    "            response = qa_invoke(conversation_memory)\n",
    "            # 假设 response 为对象，可取 response.content（否则直接用 str(response)）\n",
    "            response_text = response.content if hasattr(response, \"content\") else str(response)\n",
    "            conversation_memory.append({\"role\": \"assistant\", \"content\": response_text})\n",
    "            # 写入答案文件\n",
    "            f_out.write(response_text + \"\\n\")\n",
    "            print(f\"[{agent_name}] 已处理数据块 {idx+1}/{num_chunks}\")\n",
    "            f_out.flush()\n",
    "            time.sleep(0.1)  # 模拟处理延时\n",
    "    print(f\"[{agent_name}] 问答结果已保存至 {answer_file}\")\n",
    "\n",
    "def main():\n",
    "    pcap_file = \"input.pcap\"\n",
    "    # Step 1：分析 pcap 文件\n",
    "    log_files = analyze_pcap(pcap_file)\n",
    "\n",
    "    # Step 2：转换 log 文件为 txt 文件\n",
    "    txt_files = convert_logs(log_files, n=10, m=5)\n",
    "\n",
    "    # Step 3：筛选出有效的 txt 文件\n",
    "    effective_txt = filter_effective_txt(txt_files)\n",
    "\n",
    "    # Step 4：针对每个有效 txt 文件，进行字段提取生成 y1 文件（可能多个）\n",
    "    agent_tasks = []\n",
    "    for txt in effective_txt:\n",
    "        y1_list = process_txt_file(txt)\n",
    "        # 为每个 y1 文件生成一个任务，分配 agent_id（此处简单累加编号）\n",
    "        for y1_file in y1_list:\n",
    "            agent_tasks.append(y1_file)\n",
    "\n",
    "    print(f\"共计生成 {len(agent_tasks)} 个 agent 任务，准备并行处理...\")\n",
    "    # Step 5：并行处理每个 agent 任务\n",
    "    with ThreadPoolExecutor(max_workers=len(agent_tasks)) as executor:\n",
    "        futures = []\n",
    "        for idx, y1_file in enumerate(agent_tasks, start=1):\n",
    "            futures.append(executor.submit(process_agent, idx, y1_file))\n",
    "        for future in as_completed(futures):\n",
    "            try:\n",
    "                future.result()\n",
    "            except Exception as e:\n",
    "                print(f\"Agent 任务处理异常：{e}\")\n",
    "    print(\"所有 agent 任务均已完成。\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "【Step 1】分析 pcap 文件...\n",
      "生成的 log 文件：['capture1.log', 'capture2.log', 'capture3.log']\n",
      "【Step 2】转换 .log 文件为 .txt 文件...\n",
      "转换得到的 txt 文件：['capture1_useful.txt', 'capture2_useless.txt', 'capture3_useful.txt']\n",
      "筛选出有效 txt 文件：['capture1_useful.txt', 'capture3_useful.txt']\n",
      "【Step 3】处理 capture1_useful.txt，提取字段生成 y1 文件...\n",
      "生成的 y1 文件：['capture1_y1_1.txt', 'capture1_y1_2.txt']\n",
      "【Step 3】处理 capture3_useful.txt，提取字段生成 y1 文件...\n",
      "生成的 y1 文件：['capture3_y1.txt']\n",
      "共计生成 3 个 agent 任务，准备并行处理...\n",
      "[agent_1] 开始处理任务，源文件：capture1_y1_1.txt\n",
      "[agent_2] 开始处理任务，源文件：capture1_y1_2.txt\n",
      "[agent_3] 开始处理任务，源文件：capture3_y1.txt\n",
      "【Step 4】读取并切分文件 capture1_y1_1.txt ...\n",
      "【Step 4】读取并切分文件 capture1_y1_2.txt ...\n",
      "文件 capture1_y1_1.txt 切分为 5 个数据块\n",
      "【Step 4】读取并切分文件 capture3_y1.txt ...\n",
      "文件 capture1_y1_2.txt 切分为 5 个数据块\n",
      "文件 capture3_y1.txt 切分为 5 个数据块\n",
      "Agent 任务处理异常：Checkpointer requires one or more of the following 'configurable' keys: ['thread_id', 'checkpoint_ns', 'checkpoint_id']\n",
      "Agent 任务处理异常：Checkpointer requires one or more of the following 'configurable' keys: ['thread_id', 'checkpoint_ns', 'checkpoint_id']\n",
      "Agent 任务处理异常：Checkpointer requires one or more of the following 'configurable' keys: ['thread_id', 'checkpoint_ns', 'checkpoint_id']\n",
      "所有 agent 任务均已完成。\n"
     ]
    }
   ],
   "execution_count": 28
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
